import asyncio
import os
import re
from datetime import datetime
from typing import Dict, List

import nest_asyncio
import streamlit as st
from streamlit_tags import st_tags

# Initialize the event loop before importing crawl4ai
# flake8: noqa: E402
nest_asyncio.apply()

import tiktoken
from anthropic import Anthropic
from constants.entites_eligibilite import ENTITES
from constants.keywords import DEFAULT_KEYWORDS
from constants.tag_dp_mapping import TAG_DP_MAPPING
from dotenv import load_dotenv
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# Import pour l'Ã©valuation HITL
from services.evaluation_service import evaluation_service

# Nouveaux imports
from services.llm_services import (
    LLM_MODELS,
    MAX_TOKEN_OUTPUT,
    call_anthropic,
    call_ollama,
    call_scaleway,
)
from services.nlp_services import (
    extract_from_matches,
    extract_markdown_text,
    filter_transport_fare,
    get_matches_and_lemmas,
    load_spacy_model,
    normalize_text,
)
from sqlalchemy import create_engine, text
from star_ratings import star_ratings
from utils.crawler_utils import CrawlerManager
from utils.db_utils import get_postgres_cs

# Configuration de la page pour utiliser plus de largeur
st.set_page_config(page_title="Extraction des tags", layout="wide")

load_dotenv()

st.title("Extraction des tags")

# Connect to the database
engine = create_engine(get_postgres_cs())


def get_aom_content_by_source(siren: str, source_url: str) -> str:
    """RÃ©cupÃ¨re le contenu d'une source spÃ©cifique pour un AOM"""
    with engine.connect() as conn:
        pages = conn.execute(
            text(
                """
                SELECT url_page, contenu_scrape
                FROM tarification_raw
                WHERE n_siren_aom = :siren
                AND url_source = :url
                ORDER BY id
            """
            ),
            {"siren": siren, "url": source_url},
        ).fetchall()
        all_pages = []
        for page in pages:
            all_pages.append(
                f"--- Page: {page.url_page} ---\n{page.contenu_scrape}"
            )
    return "\n\n".join(all_pages)


def count_tokens(text: str) -> int:
    """Compte le nombre de tokens dans un texte (version gÃ©nÃ©rale)"""
    # Utiliser cl100k comme tokenizer gÃ©nÃ©ral
    return len(tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text))


# Initialiser le dictionnaire des run_ids s'il n'existe pas
if "run_ids" not in st.session_state:
    st.session_state.run_ids = {}


@traceable
def filter_nlp(
    content: str, model: str, siren: str, name: str
) -> Dict[str, str]:
    """Filtre le contenu avec SpaCy"""
    try:
        run = get_current_run_tree()
        st.session_state.run_ids["filter"] = run.id

        with st.spinner("Analyse automatique du langage naturel..."):
            nlp = load_spacy_model()
            raw_text = extract_markdown_text(content)
            paragraphs = normalize_text(raw_text, nlp)
            paragraphs_filtered, _ = filter_transport_fare(paragraphs, nlp)
            filtered = "\n\n".join(paragraphs_filtered)

            if filtered:
                return {"Contenu filtrÃ©": filtered}
            else:
                return {"Contenu filtrÃ©": ""}
    except Exception as e:
        st.error(f"Erreur de filtrage NLP : {str(e)}")
        return {"Contenu filtrÃ©": f"Erreur lors du filtrage NLP : {str(e)}"}


@traceable
def format_tags(text: str, siren: str, name: str) -> List[str]:
    """Extrait les tags uniques Ã  partir du texte"""
    run = get_current_run_tree()
    st.session_state.run_ids["format_tags"] = run.id

    nlp = load_spacy_model()

    # Obtenir les matches et lemmes
    (
        doc,
        matches_phrase,
        matches_entites,
        matches,
        tag_dp_mapping_lemmas,
    ) = get_matches_and_lemmas(text, nlp)

    # Extraire les tags
    tags_uniques, debug_matches = extract_from_matches(
        doc,
        matches_phrase,
        matches_entites,
        matches,
        tag_dp_mapping_lemmas,
        nlp,
        field="tag",
    )

    # Stocker dans session_state
    if debug_matches:
        st.session_state.tags_explanations = {
            "title": "### â„¹ï¸ Explications des tags dÃ©tectÃ©s",
            "matches": {
                tag: match_info
                for tag, match_info in debug_matches.items()
                if tag is not None
            },
        }

    return sorted(list(tag for tag in tags_uniques if tag is not None))


def show_evaluation_interface(step_name: str, content: str) -> None:
    """Affiche l'interface d'Ã©valuation pour une Ã©tape"""
    # Afficher les explications des tags si elles existent
    if step_name == "format_tags" and "tags_explanations" in st.session_state:
        st.markdown("---")
        st.markdown(st.session_state.tags_explanations["title"])
        for tag, match_info in st.session_state.tags_explanations[
            "matches"
        ].items():
            st.markdown(f"**{tag}** dÃ©tectÃ© dans :")
            st.markdown(match_info, unsafe_allow_html=True)
            st.markdown("---")

    st.divider()
    st.subheader("âœ¨ Ã‰valuation")

    # Score avec star_ratings
    stars = star_ratings("", numStars=5, key=f"stars_{step_name}")
    quality_score = stars / 5 if stars is not None else 0

    # Correction proposÃ©e
    correction = st.text_area(
        "Correction proposÃ©e (optionnel)",
        placeholder="Proposez une version corrigÃ©e du rÃ©sultat...",
        key=f"correction_{step_name}",
    )

    # Bouton de sauvegarde
    if st.button("ðŸ’¾ Sauvegarder l'Ã©valuation", key=f"save_{step_name}"):
        with st.spinner("Sauvegarde de l'Ã©valuation..."):
            run_id = st.session_state.run_ids.get(step_name)
            if not run_id:
                st.error(f"âŒ Pas de run_id trouvÃ© pour l'Ã©tape {step_name}")
                return

            feedback = evaluation_service.create_feedback(
                run_id=run_id,
                key="quality",
                score=quality_score,
                correction=correction,
            )
            if feedback:
                st.success("âœ… Ã‰valuation sauvegardÃ©e !")
            else:
                st.error("âŒ Erreur lors de la sauvegarde")


def toggle_crawling():
    if "is_crawling" not in st.session_state:
        st.session_state.is_crawling = False

    st.session_state.is_crawling = not st.session_state.is_crawling
    return st.session_state.is_crawling


# init crawler
if "crawler_manager" not in st.session_state:

    def reset_crawler_callback():
        st.session_state.crawler_manager = None

    st.session_state.crawler_manager = CrawlerManager(
        on_crawler_reset=reset_crawler_callback
    )
    st.session_state.loop = asyncio.new_event_loop()
    asyncio.set_event_loop(st.session_state.loop)


# Interface Streamlit
st.subheader("SÃ©lection de l'AOM")

# Get unique AOMs with their URLs
with engine.connect() as conn:
    aoms = conn.execute(
        text(
            """
            SELECT DISTINCT
                t.n_siren_aom,
                a.nom_aom,
                COUNT(DISTINCT t.url_source) as nb_sources,
                STRING_AGG(DISTINCT t.url_source, ' | ') as sources
            FROM tarification_raw t
            LEFT JOIN aoms a ON t.n_siren_aom = a.n_siren_aom
            GROUP BY t.n_siren_aom, a.nom_aom
            ORDER BY COUNT(DISTINCT t.url_source) DESC, a.nom_aom
            """
        )
    ).fetchall()

selected_aom = st.selectbox(
    "SÃ©lectionner une AOM:",
    options=[aom[0] for aom in aoms],
    format_func=lambda x: (
        f"{x} - "
        f"{next((a[1] for a in aoms if a[0] == x), 'Unknown')} "
        f"({next((a[2] for a in aoms if a[0] == x), 0)} sources)"
    ),
    key="selected_aom",
    on_change=lambda: (
        # Nettoyer toutes les donnÃ©es de session quand on change d'AOM
        st.session_state.pop("raw_scraped_content", None),
        st.session_state.pop("scraped_content", None),
        st.session_state.pop("filtered_contents", None),
        st.session_state.pop("tags", None),
        st.session_state.pop("run_ids", {}),  # RÃ©initialiser les run_ids
    ),
)

if selected_aom:
    n_siren_aom = next((a[0] for a in aoms if a[0] == selected_aom), None)
    nom_aom = next((a[1] for a in aoms if a[0] == selected_aom), "Nom inconnu")
    sources = next((a[3] for a in aoms if a[0] == selected_aom), "")
    st.write("Sources pour cet AOM:")
    for source in sources.split(" | "):
        st.write(f"- {source}")

    # Step 1: Scraping
    if "available_keywords" not in st.session_state:
        st.session_state.available_keywords = DEFAULT_KEYWORDS.copy()
    if "selected_keywords" not in st.session_state:
        st.session_state.selected_keywords = DEFAULT_KEYWORDS.copy()

    with st.expander("ðŸ•¸ï¸ Ã‰tape 1 : Scraper le contenu"):
        new_keyword = st.text_input(
            "Ajouter un nouveau mot-clÃ© :",
            placeholder="Entrez un nouveau mot-clÃ© et appuyez sur EntrÃ©e",
            help="Le nouveau mot-clÃ© sera ajoutÃ© Ã  la liste disponible",
        )

        if new_keyword:
            if new_keyword not in st.session_state.available_keywords:
                st.session_state.available_keywords.append(new_keyword)
                st.session_state.selected_keywords.append(new_keyword)
                st.rerun()

        selected_keywords = st.multiselect(
            "Mots-clÃ©s :",
            options=st.session_state.available_keywords,
            default=st.session_state.selected_keywords,
        )

        # Boutons existants pour dÃ©marrer/arrÃªter l'extraction
        stop_button = st.button(
            "ðŸ›‘ ArrÃªter l'extraction",
            help="Cliquez pour arrÃªter l'extraction en cours",
            disabled=not st.session_state.get("is_crawling", False),
            on_click=toggle_crawling,
        )

        start_button = st.button(
            "ðŸ•·ï¸ Lancer l'extraction",
            help="Cliquez pour lancer l'extraction des donnÃ©es sur les sites web",
            disabled=st.session_state.get("is_crawling", False),
            on_click=toggle_crawling,
        )
        if start_button:
            st.session_state.raw_scraped_content = {}
            for url_source in sources.split(" | "):
                st.session_state.raw_scraped_content[url_source] = []
                st.write(f"URL: {url_source}")
                try:
                    # Scraper l'URL
                    loop = st.session_state.loop
                    asyncio.set_event_loop(loop)
                    pages = loop.run_until_complete(
                        st.session_state.crawler_manager.fetch_content(
                            url_source,
                            st.session_state.selected_keywords,
                        )
                    )
                    # Ajouter les pages Ã  la liste globale
                    for page in pages:
                        st.session_state.raw_scraped_content[
                            url_source
                        ].append(
                            {
                                "url": page.url,
                                "markdown": page.markdown,
                            }
                        )
                except Exception as e:
                    st.error(f"âš ï¸ Une erreur est survenue : {str(e)}")
            st.session_state.is_crawling = False
            st.rerun()

    # Step 2: Affichage du contenu scrapÃ©
    with st.expander("ðŸ‘€ Ã‰tape 2 : Afficher le contenu scrapÃ©"):
        if (
            "raw_scraped_content" in st.session_state
            and st.session_state.raw_scraped_content
        ):
            # Utiliser les donnÃ©es en session
            sources = st.session_state.raw_scraped_content

            # PrÃ©parer le contenu total pour compter les tokens
            scraped_content = ""
            for pages in sources.values():
                for page in pages:
                    scraped_content += (
                        f"--- Page: {page['url']} ---\n{page['markdown']}\n\n"
                    )

            nb_tokens = count_tokens(scraped_content)
            st.write(f"Nombre de tokens : {nb_tokens}")

            # Afficher le contenu par source
            tabs = st.tabs([f"Source {i+1}" for i in range(len(sources))])

            for i, (url_source, pages) in enumerate(sources.items()):
                with tabs[i]:
                    st.write(f"Source {i+1}")
                    st.write(
                        f"Date d'extraction: {datetime.now().strftime('%Y-%m-%d')}"
                    )
                    st.write(f"URL source: {url_source}")
                    # Afficher chaque page de la source
                    for page in pages:
                        st.write(f"URL: {page['url']}")
                        st.markdown(page["markdown"])

            # Sauvegarder dans session_state pour les Ã©tapes suivantes
            st.session_state.scraped_content = scraped_content

        else:
            # Fallback sur la base de donnÃ©es (code existant)
            sources = next(
                (a[3] for a in aoms if a[0] == selected_aom), ""
            ).split(" | ")
            scraped_content = ""
            for i, source in enumerate(sources):
                content = get_aom_content_by_source(selected_aom, source)
                scraped_content += content + "\n\n"
            nb_tokens = count_tokens(scraped_content)
            st.write(f"Nombre de tokens : {nb_tokens}")
            sources_content = {}
            tabs = st.tabs([f"Source {i+1}" for i in range(len(sources))])

            # ConcatÃ©ner le contenu de toutes les sources
            for i, source in enumerate(sources):
                with tabs[i]:
                    st.write(f"URL: {source}")
                    content = get_aom_content_by_source(selected_aom, source)
                    sources_content[source] = content
                    st.markdown(content)

            # Sauvegarder dans session_state pour les Ã©tapes suivantes
            st.session_state.scraped_content = scraped_content

    # Step 3: Filtrage du contenu
    with st.expander("ðŸŽ¯ Ã‰tape 3 : Filtrage du contenu"):
        # VÃ©rifier si l'Ã©tape prÃ©cÃ©dente est complÃ©tÃ©e
        is_previous_step_complete = (
            "scraped_content" in st.session_state
            and st.session_state.scraped_content
        )

        if not is_previous_step_complete:
            st.warning("âš ï¸ Veuillez d'abord complÃ©ter l'Ã©tape de scraping")

        # Afficher le contenu filtrÃ© s'il existe
        if "filtered_contents" in st.session_state:
            filtered_content = st.session_state.filtered_contents[
                "Contenu filtrÃ©"
            ]
            nb_tokens = count_tokens(filtered_content)

            st.text_area(
                label=f"Contenu filtrÃ© (NLP) - {nb_tokens} tokens",
                value=filtered_content,
                height=500,
                disabled=True,
            )
            # Ajouter l'interface d'Ã©valuation
            show_evaluation_interface("filter", filtered_content)

        if st.button(
            "Lancer le filtrage",
            key="filter_content",
            disabled=not is_previous_step_complete,
        ):
            # VÃ©rification du contenu une seule fois
            scraped_content = st.session_state.get("scraped_content", {})
            if not scraped_content:
                st.error("Veuillez d'abord charger le contenu dans l'Ã©tape 2")
                st.stop()
            filtered_result = filter_nlp(
                scraped_content,
                "custom_filter_v2",
                n_siren_aom,
                nom_aom,
            )
            if filtered_result["Contenu filtrÃ©"].strip():
                st.session_state.filtered_contents = filtered_result
                st.success("Filtrage terminÃ©")
                st.rerun()
            else:
                st.error("Aucun contenu pertinent trouvÃ© dans les sources")

    # Step 4: Extraction des tags

    with st.expander("Ã‰tape 4 : Extraction des tags", expanded=True):
        is_previous_step_complete = (
            "filtered_contents" in st.session_state
            and st.session_state.filtered_contents.get(
                "Contenu filtrÃ©", ""
            ).strip()
        )

        if not is_previous_step_complete:
            st.warning("âš ï¸ Veuillez d'abord complÃ©ter l'Ã©tape de filtrage")
        if st.button(
            "Extraire les tags",
            key="format_in_tags",
            use_container_width=True,
            disabled=not is_previous_step_complete,
        ):
            with st.spinner("GÃ©nÃ©ration des tags en cours..."):
                # Extraire les tags et data providers
                st.session_state.tags = format_tags(
                    st.session_state.filtered_contents[
                        "Contenu filtrÃ©"
                    ].strip(),
                    n_siren_aom,
                    nom_aom,
                )
                st.rerun()

        # Afficher les tags s'ils existent dans la session
        if "tags" in st.session_state:
            st.session_state.tags = st_tags(
                label="# Tags dÃ©tectÃ©s :",
                text="",
                value=st.session_state.tags,
                key="tag_display",
            )
            show_evaluation_interface("format_tags", st.session_state.tags)

            # Afficher les fournisseurs associÃ©s aux tags
            st.markdown("### ðŸ¢ Fournisseurs de donnÃ©es")
            st.markdown(
                "Les fournisseurs suivants ont Ã©tÃ© identifiÃ©s pour les tags dÃ©tectÃ©s :"
            )

            # CrÃ©er un dictionnaire tag -> fournisseur Ã  partir des matches
            tag_provider_map = {}
            for tag in st.session_state.tags:
                providers = set()
                for k, v in TAG_DP_MAPPING.items():
                    if v.get("tag") == tag and v.get("fournisseur"):
                        providers.add(v["fournisseur"])
                if providers:
                    tag_provider_map[tag] = sorted(list(providers))

            # Afficher les associations tag -> fournisseur
            for tag, providers in tag_provider_map.items():
                st.markdown(f"**{tag}** : {', '.join(providers)}")
