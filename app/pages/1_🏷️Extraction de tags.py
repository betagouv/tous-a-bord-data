import asyncio
import os
import re
from datetime import datetime
from typing import Dict, List

import nest_asyncio
import streamlit as st
from streamlit_tags import st_tags

# Initialize the event loop before importing crawl4ai
# flake8: noqa: E402
nest_asyncio.apply()

import tiktoken
from anthropic import Anthropic
from constants.entites_eligibilite import ENTITES
from constants.keywords import DEFAULT_KEYWORDS
from constants.tag_dp_mapping import TAG_DP_MAPPING
from dotenv import load_dotenv
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# Import pour l'√©valuation HITL
from services.evaluation_service import evaluation_service

# Nouveaux imports
from services.llm_services import (
    LLM_MODELS,
    MAX_TOKEN_OUTPUT,
    call_anthropic,
    call_ollama,
    call_scaleway,
)
from services.nlp_services import (
    create_eligibility_matcher,
    extract_markdown_text,
    filter_transport_fare,
    load_spacy_model,
    normalize_text,
)
from sqlalchemy import create_engine, text
from star_ratings import star_ratings
from utils.crawler_utils import CrawlerManager
from utils.db_utils import get_postgres_cs

# Configuration de la page pour utiliser plus de largeur
st.set_page_config(page_title="Extraction des tags", layout="wide")

load_dotenv()

st.title("Extraction des tags")

# Connect to the database
engine = create_engine(get_postgres_cs())


def get_aom_content_by_source(siren: str, source_url: str) -> str:
    """R√©cup√®re le contenu d'une source sp√©cifique pour un AOM"""
    with engine.connect() as conn:
        pages = conn.execute(
            text(
                """
                SELECT url_page, contenu_scrape
                FROM tarification_raw
                WHERE n_siren_aom = :siren
                AND url_source = :url
                ORDER BY id
            """
            ),
            {"siren": siren, "url": source_url},
        ).fetchall()
        all_pages = []
        for page in pages:
            all_pages.append(
                f"--- Page: {page.url_page} ---\n{page.contenu_scrape}"
            )
    return "\n\n".join(all_pages)


def count_tokens(text: str) -> int:
    """Compte le nombre de tokens dans un texte (version g√©n√©rale)"""
    # Utiliser cl100k comme tokenizer g√©n√©ral
    return len(tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text))


# Initialiser le dictionnaire des run_ids s'il n'existe pas
if "run_ids" not in st.session_state:
    st.session_state.run_ids = {}


@traceable
def filter_nlp(
    content: str, model: str, siren: str, name: str
) -> Dict[str, str]:
    """Filtre le contenu avec SpaCy"""
    try:
        run = get_current_run_tree()
        st.session_state.run_ids["filter"] = run.id

        with st.spinner(
            "Chargement du mod√®le d'analyse automatique du langage..."
        ):
            nlp = load_spacy_model()
            raw_text = extract_markdown_text(content)
            paragraphs = normalize_text(raw_text, nlp)
            paragraphs_filtered, _ = filter_transport_fare(paragraphs, nlp)
            filtered = "\n\n".join(paragraphs_filtered)

            if filtered:
                return {"Contenu filtr√©": filtered}
            else:
                return {"Contenu filtr√©": ""}
    except Exception as e:
        st.error(f"Erreur de filtrage NLP : {str(e)}")
        return {"Contenu filtr√©": f"Erreur lors du filtrage NLP : {str(e)}"}


@traceable
def format_tags(text: str, nlp, siren: str, name: str) -> List[str]:
    """Extrait les tags uniques √† partir du texte"""
    run = get_current_run_tree()
    st.session_state.run_ids["format_tags"] = run.id
    # Cr√©er le matcher
    phrase_matcher, matcher = create_eligibility_matcher(nlp)

    # Traiter le texte
    doc = nlp(text)
    # Chercher les crit√®res dans tout le paragraphe
    matches_phrase = phrase_matcher(doc)
    matches_entites = any(token.text in ENTITES for token in doc)

    # Trouver les correspondances et r√©cup√©rer les tags uniques
    tags_uniques = set()
    debug_matches = {}  # Pour stocker les correspondances texte -> tag

    # Cr√©er un dictionnaire avec les cl√©s en minuscules et leurs lemmes
    tag_dp_mapping_lemmas = {}
    for k, v in TAG_DP_MAPPING.items():
        if k and v and v.get("tag"):  # V√©rifier que la cl√© et le tag existent
            doc_key = nlp(k.lower())
            lemma = doc_key[
                0
            ].lemma_.lower()  # Prendre le lemme du premier token
            tag_dp_mapping_lemmas[lemma] = v

    # Pour les matches de phrases
    for match_id, start, end in matches_phrase:
        span = doc[start:end]
        if not span.text:  # V√©rifier que le span n'est pas vide
            continue
        span_doc = nlp(span.text.lower())
        span_lemma = span_doc[0].lemma_.lower()
        if span_lemma in tag_dp_mapping_lemmas:
            tag = tag_dp_mapping_lemmas[span_lemma]["tag"]
            if tag:  # V√©rifier que le tag n'est pas None
                tags_uniques.add(tag)
                if tag not in debug_matches:
                    debug_matches[tag] = []
                debug_matches[tag].append(
                    f"Phrase: '{span.text}' (lemme: {span_lemma})"
                )

    # Pour les entit√©s
    if matches_entites:
        for token in doc:
            if token.text in ENTITES:
                token_lemma = token.lemma_.lower()
                if token_lemma in tag_dp_mapping_lemmas:
                    tag = tag_dp_mapping_lemmas[token_lemma]["tag"]
                    if tag:  # V√©rifier que le tag n'est pas None
                        tags_uniques.add(tag)
                        if tag not in debug_matches:
                            debug_matches[tag] = []
                        debug_matches[tag].append(
                            f"Entit√©: '{token.text}' (lemme: {token_lemma})"
                        )

    # Pour les matchs sp√©ciaux (AGE et QF)
    matches = matcher(doc)
    special_tags = {"AGE": "Age", "QF": "Quotient Familial"}
    for match_id, start, end in matches:
        match_type = nlp.vocab.strings[match_id]
        if match_type in special_tags:
            tag = special_tags[match_type]
            if tag:  # V√©rifier que le tag n'est pas None
                span = doc[start:end]
                tags_uniques.add(tag)
                if tag not in debug_matches:
                    debug_matches[tag] = []
                debug_matches[tag].append(
                    f"Match sp√©cial {match_type}: '{span.text}'"
                )

    # Afficher les correspondances pour le debugging
    if debug_matches:
        st.write("### üîç D√©tails des correspondances trouv√©es:")
        for tag in sorted(
            tag for tag in debug_matches.keys() if tag is not None
        ):
            st.markdown(f"**Tag : {tag}**")
            for match in debug_matches[tag]:
                st.markdown(f"- {match}")
            st.markdown("---")

    return sorted(list(tag for tag in tags_uniques if tag is not None))


def show_evaluation_interface(step_name: str, content: str) -> None:
    """Affiche l'interface d'√©valuation pour une √©tape"""
    st.divider()
    st.subheader("‚ú® √âvaluation")

    # Score avec star_ratings
    stars = star_ratings("", numStars=5, key=f"stars_{step_name}")
    quality_score = stars / 5 if stars is not None else 0

    # Correction propos√©e
    correction = st.text_area(
        "Correction propos√©e (optionnel)",
        placeholder="Proposez une version corrig√©e du r√©sultat...",
        key=f"correction_{step_name}",
    )

    # Bouton de sauvegarde
    if st.button("üíæ Sauvegarder l'√©valuation", key=f"save_{step_name}"):
        with st.spinner("Sauvegarde de l'√©valuation..."):
            run_id = st.session_state.run_ids.get(step_name)
            if not run_id:
                st.error(f"‚ùå Pas de run_id trouv√© pour l'√©tape {step_name}")
                return

            feedback = evaluation_service.create_feedback(
                run_id=run_id,
                key="quality",
                score=quality_score,
                correction=correction,
            )
            if feedback:
                st.success("‚úÖ √âvaluation sauvegard√©e !")
            else:
                st.error("‚ùå Erreur lors de la sauvegarde")


def toggle_crawling():
    if "is_crawling" not in st.session_state:
        st.session_state.is_crawling = False

    st.session_state.is_crawling = not st.session_state.is_crawling
    return st.session_state.is_crawling


# init crawler
if "crawler_manager" not in st.session_state:

    def reset_crawler_callback():
        st.session_state.crawler_manager = None

    st.session_state.crawler_manager = CrawlerManager(
        on_crawler_reset=reset_crawler_callback
    )
    st.session_state.loop = asyncio.new_event_loop()
    asyncio.set_event_loop(st.session_state.loop)


# Interface Streamlit
st.subheader("S√©lection de l'AOM")

# Get unique AOMs with their URLs
with engine.connect() as conn:
    aoms = conn.execute(
        text(
            """
            SELECT DISTINCT
                t.n_siren_aom,
                a.nom_aom,
                COUNT(DISTINCT t.url_source) as nb_sources,
                STRING_AGG(DISTINCT t.url_source, ' | ') as sources
            FROM tarification_raw t
            LEFT JOIN aoms a ON t.n_siren_aom = a.n_siren_aom
            GROUP BY t.n_siren_aom, a.nom_aom
            ORDER BY COUNT(DISTINCT t.url_source) DESC, a.nom_aom
            """
        )
    ).fetchall()

selected_aom = st.selectbox(
    "S√©lectionner une AOM:",
    options=[aom[0] for aom in aoms],
    format_func=lambda x: (
        f"{x} - "
        f"{next((a[1] for a in aoms if a[0] == x), 'Unknown')} "
        f"({next((a[2] for a in aoms if a[0] == x), 0)} sources)"
    ),
    key="selected_aom",
    on_change=lambda: (
        # Nettoyer toutes les donn√©es de session quand on change d'AOM
        st.session_state.pop("raw_scraped_content", None),
        st.session_state.pop("scraped_content", None),
        st.session_state.pop("filtered_contents", None),
        st.session_state.pop("tags", None),
        st.session_state.pop("run_ids", {}),  # R√©initialiser les run_ids
    ),
)

if selected_aom:
    n_siren_aom = next((a[0] for a in aoms if a[0] == selected_aom), None)
    nom_aom = next((a[1] for a in aoms if a[0] == selected_aom), "Nom inconnu")
    sources = next((a[3] for a in aoms if a[0] == selected_aom), "")
    st.write("Sources pour cet AOM:")
    for source in sources.split(" | "):
        st.write(f"- {source}")

    # Step 1: Scraping
    if "available_keywords" not in st.session_state:
        st.session_state.available_keywords = DEFAULT_KEYWORDS.copy()
    if "selected_keywords" not in st.session_state:
        st.session_state.selected_keywords = DEFAULT_KEYWORDS.copy()

    with st.expander("üï∏Ô∏è √âtape 1 : Scraper le contenu"):
        new_keyword = st.text_input(
            "Ajouter un nouveau mot-cl√© :",
            placeholder="Entrez un nouveau mot-cl√© et appuyez sur Entr√©e",
            help="Le nouveau mot-cl√© sera ajout√© √† la liste disponible",
        )

        if new_keyword:
            if new_keyword not in st.session_state.available_keywords:
                st.session_state.available_keywords.append(new_keyword)
                st.session_state.selected_keywords.append(new_keyword)
                st.rerun()

        selected_keywords = st.multiselect(
            "Mots-cl√©s :",
            options=st.session_state.available_keywords,
            default=st.session_state.selected_keywords,
        )

        # Boutons existants pour d√©marrer/arr√™ter l'extraction
        stop_button = st.button(
            "üõë Arr√™ter l'extraction",
            help="Cliquez pour arr√™ter l'extraction en cours",
            disabled=not st.session_state.get("is_crawling", False),
            on_click=toggle_crawling,
        )

        start_button = st.button(
            "üï∑Ô∏è Lancer l'extraction",
            help="Cliquez pour lancer l'extraction des donn√©es sur les sites web",
            disabled=st.session_state.get("is_crawling", False),
            on_click=toggle_crawling,
        )
        if start_button:
            st.session_state.raw_scraped_content = {}
            for url_source in sources.split(" | "):
                st.session_state.raw_scraped_content[url_source] = []
                st.write(f"URL: {url_source}")
                try:
                    # Scraper l'URL
                    loop = st.session_state.loop
                    asyncio.set_event_loop(loop)
                    pages = loop.run_until_complete(
                        st.session_state.crawler_manager.fetch_content(
                            url_source,
                            st.session_state.selected_keywords,
                        )
                    )
                    # Ajouter les pages √† la liste globale
                    for page in pages:
                        st.session_state.raw_scraped_content[
                            url_source
                        ].append(
                            {
                                "url": page.url,
                                "markdown": page.markdown,
                            }
                        )
                except Exception as e:
                    st.error(f"‚ö†Ô∏è Une erreur est survenue : {str(e)}")
            st.session_state.is_crawling = False
            st.rerun()

    # Step 2: Affichage du contenu scrap√©
    with st.expander("üëÄ √âtape 2 : Afficher le contenu scrap√©"):
        if (
            "raw_scraped_content" in st.session_state
            and st.session_state.raw_scraped_content
        ):
            # Utiliser les donn√©es en session
            sources = st.session_state.raw_scraped_content

            # Pr√©parer le contenu total pour compter les tokens
            scraped_content = ""
            for pages in sources.values():
                for page in pages:
                    scraped_content += (
                        f"--- Page: {page['url']} ---\n{page['markdown']}\n\n"
                    )

            nb_tokens = count_tokens(scraped_content)
            st.write(f"Nombre de tokens : {nb_tokens}")

            # Afficher le contenu par source
            tabs = st.tabs([f"Source {i+1}" for i in range(len(sources))])

            for i, (url_source, pages) in enumerate(sources.items()):
                with tabs[i]:
                    st.write(f"Source {i+1}")
                    st.write(
                        f"Date d'extraction: {datetime.now().strftime('%Y-%m-%d')}"
                    )
                    st.write(f"URL source: {url_source}")
                    # Afficher chaque page de la source
                    for page in pages:
                        st.write(f"URL: {page['url']}")
                        st.markdown(page["markdown"])

            # Sauvegarder dans session_state pour les √©tapes suivantes
            st.session_state.scraped_content = scraped_content

        else:
            # Fallback sur la base de donn√©es (code existant)
            sources = next(
                (a[3] for a in aoms if a[0] == selected_aom), ""
            ).split(" | ")
            scraped_content = ""
            for i, source in enumerate(sources):
                content = get_aom_content_by_source(selected_aom, source)
                scraped_content += content + "\n\n"
            nb_tokens = count_tokens(scraped_content)
            st.write(f"Nombre de tokens : {nb_tokens}")
            sources_content = {}
            tabs = st.tabs([f"Source {i+1}" for i in range(len(sources))])

            # Concat√©ner le contenu de toutes les sources
            for i, source in enumerate(sources):
                with tabs[i]:
                    st.write(f"URL: {source}")
                    content = get_aom_content_by_source(selected_aom, source)
                    sources_content[source] = content
                    st.markdown(content)

            # Sauvegarder dans session_state pour les √©tapes suivantes
            st.session_state.scraped_content = scraped_content

    # Step 3: Filtrage du contenu
    with st.expander("üéØ √âtape 3 : Filtrage du contenu"):
        # V√©rifier si l'√©tape pr√©c√©dente est compl√©t√©e
        is_previous_step_complete = (
            "scraped_content" in st.session_state
            and st.session_state.scraped_content
        )

        if not is_previous_step_complete:
            st.warning("‚ö†Ô∏è Veuillez d'abord compl√©ter l'√©tape de scraping")

        # Afficher le contenu filtr√© s'il existe
        if "filtered_contents" in st.session_state:
            filtered_content = st.session_state.filtered_contents[
                "Contenu filtr√©"
            ]
            nb_tokens = count_tokens(filtered_content)

            st.text_area(
                label=f"Contenu filtr√© (NLP) - {nb_tokens} tokens",
                value=filtered_content,
                height=500,
                disabled=True,
            )
            # Ajouter l'interface d'√©valuation
            show_evaluation_interface("filter", filtered_content)

        if st.button(
            "Lancer le filtrage",
            key="filter_content",
            disabled=not is_previous_step_complete,
        ):
            # V√©rification du contenu une seule fois
            scraped_content = st.session_state.get("scraped_content", {})
            if not scraped_content:
                st.error("Veuillez d'abord charger le contenu dans l'√©tape 2")
                st.stop()
            filtered_result = filter_nlp(
                scraped_content,
                "custom_filter_v2",
                n_siren_aom,
                nom_aom,
            )
            if filtered_result["Contenu filtr√©"].strip():
                st.session_state.filtered_contents = filtered_result
                st.success("Filtrage termin√©")
                st.rerun()
            else:
                st.error("Aucun contenu pertinent trouv√© dans les sources")

    # Step 4: Extraction des tags

    with st.expander("√âtape 4 : Extraction des tags", expanded=True):
        is_previous_step_complete = (
            "filtered_contents" in st.session_state
            and st.session_state.filtered_contents.get(
                "Contenu filtr√©", ""
            ).strip()
        )

        if not is_previous_step_complete:
            st.warning("‚ö†Ô∏è Veuillez d'abord compl√©ter l'√©tape de filtrage")
        if st.button(
            "Extraire les tags",
            key="format_in_tags",
            use_container_width=True,
            disabled=not is_previous_step_complete,
        ):
            with st.spinner("G√©n√©ration des tags en cours..."):
                # Charger le mod√®le SpaCy
                nlp = load_spacy_model()
                # Extraire les tags et data providers
                st.session_state.tags = format_tags(
                    st.session_state.filtered_contents[
                        "Contenu filtr√©"
                    ].strip(),
                    nlp,
                    n_siren_aom,
                    nom_aom,
                )
                st.rerun()

        # Afficher les tags s'ils existent dans la session
        if "tags" in st.session_state:
            st.session_state.tags = st_tags(
                label="# Tags d√©tect√©s :",
                text="",
                value=st.session_state.tags,
                key="tag_display",
            )
            show_evaluation_interface("format_tags", st.session_state.tags)
