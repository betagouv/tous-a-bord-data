import pandas as pd
import streamlit as st
from services.evaluation_service import evaluation_service

st.set_page_config(
    page_title="√âvaluation HITL - Transport Tarifs",
    page_icon="üìä",
    layout="wide",
)

st.title("üìä √âvaluation Human-in-the-Loop (HITL)")
st.markdown("Interface d'√©valuation pour am√©liorer la qualit√© du pipeline RAG")

# Sidebar pour la navigation
st.sidebar.title("Navigation")
mode = st.sidebar.selectbox(
    "Mode d'√©valuation",
    [
        "üìà Tableau de bord",
        "üîç √âvaluer les runs",
        "üìã Historique des √©valuations",
    ],
)

if mode == "üìà Tableau de bord":
    st.header("Tableau de bord des √©valuations")

    # R√©cup√©rer les statistiques
    with st.spinner("Chargement des statistiques..."):
        stats = evaluation_service.get_evaluation_stats()

    # Afficher les m√©triques principales
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            "Total des runs",
            stats["total_runs"],
            help="Nombre total de runs LLM ex√©cut√©s",
        )

    with col2:
        st.metric(
            "Runs √©valu√©s",
            stats["runs_with_feedback"],
            help="Nombre de runs ayant re√ßu un feedback humain",
        )

    with col3:
        coverage_pct = f"{stats['feedback_coverage']:.1%}"
        st.metric(
            "Couverture d'√©valuation",
            coverage_pct,
            help="Pourcentage de runs √©valu√©s par un humain",
        )

    with col4:
        avg_score = f"{stats['average_score']:.2f}"
        st.metric(
            "Score moyen", avg_score, help="Score moyen des √©valuations (0-1)"
        )

    # Graphique de progression (placeholder pour l'instant)
    st.subheader("√âvolution de la qualit√©")
    st.info(
        "üìä Graphiques de progression √† venir - n√©cessite plus de "
        "donn√©es historiques"
    )

    # Recommandations
    st.subheader("Recommandations")
    if stats["feedback_coverage"] < 0.2:
        st.warning(
            "‚ö†Ô∏è Couverture d'√©valuation faible. √âvaluez plus de runs "
            "pour am√©liorer le syst√®me."
        )
    elif stats["average_score"] < 0.7:
        st.warning(
            "‚ö†Ô∏è Score moyen faible. Analysez les runs mal not√©s pour "
            "identifier les probl√®mes."
        )
    else:
        st.success(
            "‚úÖ Bon niveau d'√©valuation. Continuez √† √©valuer " "r√©guli√®rement."
        )

elif mode == "üîç √âvaluer les runs":
    st.header("√âvaluation des runs LLM")

    # Filtres
    col1, col2 = st.columns(2)
    with col1:
        limit = st.number_input(
            "Nombre de runs √† afficher", min_value=10, max_value=100, value=20
        )
    with col2:
        show_only_unevaluated = st.checkbox(
            "Afficher seulement les runs non √©valu√©s", value=True
        )

    # R√©cup√©rer les runs
    with st.spinner("Chargement des runs..."):
        runs = evaluation_service.list_recent_runs(limit=limit)

    if not runs:
        st.warning(
            "Aucun run trouv√©. Assurez-vous que LangSmith est "
            "correctement configur√©."
        )
        st.stop()

    # Filtrer les runs non √©valu√©s si demand√©
    if show_only_unevaluated:
        unevaluated_runs = []
        for run in runs:
            feedbacks = evaluation_service.get_feedback_for_run(run.id)
            if not feedbacks:
                unevaluated_runs.append(run)
        runs = unevaluated_runs

    if not runs:
        st.info("‚úÖ Tous les runs r√©cents ont √©t√© √©valu√©s !")
        st.stop()

    st.write(f"**{len(runs)} runs** √† √©valuer")

    # Interface d'√©valuation pour chaque run
    for i, run in enumerate(runs):
        run_title = (
            f"Run {i+1}: {run.name or 'Sans nom'} - "
            f"{run.start_time.strftime('%Y-%m-%d %H:%M')}"
        )
        with st.expander(run_title):

            # Informations du run
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**ID:** `{run.id}`")
                model_name = run.extra.get("metadata", {}).get(
                    "model", "Non sp√©cifi√©"
                )
                st.write(f"**Mod√®le:** {model_name}")
                st.write(
                    f"**Dur√©e:** {run.execution_order}ms"
                    if run.execution_order
                    else "N/A"
                )

            with col2:
                st.write(f"**Statut:** {run.status}")
                st.write(f"**Type:** {run.run_type}")
                if run.error:
                    st.error(f"**Erreur:** {run.error}")

            # Inputs et outputs
            if run.inputs:
                st.subheader("üì• Entr√©es")
                for key, value in run.inputs.items():
                    if isinstance(value, str) and len(value) > 500:
                        st.text_area(
                            f"**{key}:**",
                            value[:500] + "...",
                            height=100,
                            disabled=True,
                        )
                    else:
                        st.write(f"**{key}:** {value}")

            if run.outputs:
                st.subheader("üì§ Sorties")
                for key, value in run.outputs.items():
                    if isinstance(value, str):
                        st.text_area(
                            f"**{key}:**", value, height=150, disabled=True
                        )
                    else:
                        st.write(f"**{key}:** {value}")

            # Interface d'√©valuation
            st.subheader("üìä √âvaluation")

            eval_col1, eval_col2, eval_col3 = st.columns(3)

            with eval_col1:
                quality_score = st.select_slider(
                    "Qualit√© g√©n√©rale",
                    options=[0, 0.25, 0.5, 0.75, 1.0],
                    format_func=lambda x: {
                        0: "Tr√®s mauvais",
                        0.25: "Mauvais",
                        0.5: "Moyen",
                        0.75: "Bon",
                        1.0: "Excellent",
                    }[x],
                    key=f"quality_{run.id}",
                )

            with eval_col2:
                relevance_score = st.select_slider(
                    "Pertinence",
                    options=[0, 0.25, 0.5, 0.75, 1.0],
                    format_func=lambda x: {
                        0: "Non pertinent",
                        0.25: "Peu pertinent",
                        0.5: "Moyennement pertinent",
                        0.75: "Pertinent",
                        1.0: "Tr√®s pertinent",
                    }[x],
                    key=f"relevance_{run.id}",
                )

            with eval_col3:
                accuracy_score = st.select_slider(
                    "Pr√©cision",
                    options=[0, 0.25, 0.5, 0.75, 1.0],
                    format_func=lambda x: {
                        0: "Tr√®s impr√©cis",
                        0.25: "Impr√©cis",
                        0.5: "Moyennement pr√©cis",
                        0.75: "Pr√©cis",
                        1.0: "Tr√®s pr√©cis",
                    }[x],
                    key=f"accuracy_{run.id}",
                )

            # Commentaires et corrections
            comment = st.text_area(
                "Commentaires (optionnel)",
                placeholder=(
                    "D√©crivez les probl√®mes identifi√©s ou les "
                    "am√©liorations possibles..."
                ),
                key=f"comment_{run.id}",
            )

            correction = st.text_area(
                "Correction propos√©e (optionnel)",
                placeholder="Proposez une version corrig√©e de la sortie...",
                key=f"correction_{run.id}",
            )

            # Bouton de soumission
            if st.button("üíæ Sauvegarder l'√©valuation", key=f"submit_{run.id}"):
                with st.spinner("Sauvegarde en cours..."):
                    # Cr√©er les feedbacks
                    feedback_ids = []

                    # Feedback qualit√©
                    if quality_score is not None:
                        fid = evaluation_service.create_feedback(
                            run_id=run.id,
                            key="quality",
                            score=quality_score,
                            comment=comment if comment else None,
                        )
                        if fid:
                            feedback_ids.append(fid)

                    # Feedback pertinence
                    if relevance_score is not None:
                        fid = evaluation_service.create_feedback(
                            run_id=run.id,
                            key="relevance",
                            score=relevance_score,
                            comment=comment if comment else None,
                        )
                        if fid:
                            feedback_ids.append(fid)

                    # Feedback pr√©cision
                    if accuracy_score is not None:
                        fid = evaluation_service.create_feedback(
                            run_id=run.id,
                            key="accuracy",
                            score=accuracy_score,
                            comment=comment if comment else None,
                            correction={"corrected_output": correction}
                            if correction
                            else None,
                        )
                        if fid:
                            feedback_ids.append(fid)

                    if feedback_ids:
                        success_msg = (
                            f"‚úÖ √âvaluation sauvegard√©e ! "
                            f"({len(feedback_ids)} feedbacks cr√©√©s)"
                        )
                        st.success(success_msg)
                        st.rerun()
                    else:
                        st.error("‚ùå Erreur lors de la sauvegarde")

elif mode == "üìã Historique des √©valuations":
    st.header("Historique des √©valuations")

    # R√©cup√©rer les runs avec feedback
    with st.spinner("Chargement de l'historique..."):
        runs = evaluation_service.list_recent_runs(limit=50)

        # Cr√©er un tableau avec les √©valuations
        evaluation_data = []
        for run in runs:
            feedbacks = evaluation_service.get_feedback_for_run(run.id)
            if feedbacks:
                for feedback in feedbacks:
                    evaluation_data.append(
                        {
                            "Date": run.start_time.strftime("%Y-%m-%d %H:%M"),
                            "Run ID": run.id[:8] + "...",
                            "Nom": run.name or "Sans nom",
                            "Type d'√©valuation": feedback.key,
                            "Score": feedback.score,
                            "Commentaire": feedback.comment[:100] + "..."
                            if feedback.comment and len(feedback.comment) > 100
                            else feedback.comment,
                            "Correction": "Oui"
                            if feedback.correction
                            else "Non",
                        }
                    )

    if evaluation_data:
        df = pd.DataFrame(evaluation_data)

        # Filtres
        col1, col2 = st.columns(2)
        with col1:
            eval_types = st.multiselect(
                "Types d'√©valuation",
                options=df["Type d'√©valuation"].unique(),
                default=df["Type d'√©valuation"].unique(),
            )

        with col2:
            min_score = st.slider("Score minimum", 0.0, 1.0, 0.0, 0.25)

        # Filtrer le dataframe
        filtered_df = df[
            (df["Type d'√©valuation"].isin(eval_types))
            & (df["Score"] >= min_score)
        ]

        # Afficher le tableau
        st.dataframe(filtered_df, use_container_width=True, hide_index=True)

        # Statistiques rapides
        if not filtered_df.empty:
            st.subheader("Statistiques")
            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Nombre d'√©valuations", len(filtered_df))

            with col2:
                avg_score = filtered_df["Score"].mean()
                st.metric("Score moyen", f"{avg_score:.2f}")

            with col3:
                corrections_count = len(
                    filtered_df[filtered_df["Correction"] == "Oui"]
                )
                st.metric("Corrections propos√©es", corrections_count)

    else:
        st.info(
            "Aucune √©valuation trouv√©e. Commencez par √©valuer "
            "quelques runs !"
        )

# Footer avec informations de configuration
st.sidebar.markdown("---")
st.sidebar.subheader("Configuration LangSmith")
if evaluation_service.client:
    st.sidebar.success("‚úÖ Connect√© √† LangSmith")
    st.sidebar.write(f"**Projet:** {evaluation_service.project_name}")
else:
    st.sidebar.error("‚ùå Erreur de connexion LangSmith")
    st.sidebar.write("V√©rifiez vos variables d'environnement :")
    st.sidebar.code(
        """
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_key
LANGCHAIN_PROJECT=transport-tarifs-pipeline
    """
    )
