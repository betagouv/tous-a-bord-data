import asyncio

import nest_asyncio
import streamlit as st

# Initialize the event loop before importing crawl4ai
# flake8: noqa: E402
nest_asyncio.apply()
from datetime import datetime, timedelta
from urllib.parse import urlparse

from sqlalchemy import create_engine, text
from utils.crawler_utils import CrawlerManager
from utils.db_utils import get_postgres_cs, load_urls_data_from_db

st.title("Extraction multipages sur les AOMs")


def toggle_crawling():
    if "is_crawling" not in st.session_state:
        st.session_state.is_crawling = False

    st.session_state.is_crawling = not st.session_state.is_crawling
    return st.session_state.is_crawling


# retrieve urls from db
urls_data = load_urls_data_from_db()
DATA = urls_data[["n_siren_aom", "nom_aom", "site_web_principal"]].to_dict(
    orient="records"
)
# init crawler
if "crawler_manager" not in st.session_state:

    def reset_crawler_callback():
        st.session_state.crawler_manager = None

    st.session_state.crawler_manager = CrawlerManager(
        on_crawler_reset=reset_crawler_callback
    )
    st.session_state.loop = asyncio.new_event_loop()
    asyncio.set_event_loop(st.session_state.loop)

# Ajout des tags de mots-cl√©s
default_keywords = [
    "offres",
    "boutique",
    "tarif",
    "abonnement",
    "solidaire",
    "acheter",
    "pass",
    "titre",
]
if "available_keywords" not in st.session_state:
    st.session_state.available_keywords = default_keywords.copy()
if "selected_keywords" not in st.session_state:
    st.session_state.selected_keywords = default_keywords.copy()

# Ajout d'un champ pour les mots-cl√©s personnalis√©s
new_keyword = st.text_input(
    label="Ajouter un nouveau mot-cl√© :",
    placeholder="Entrez un nouveau mot-cl√© et appuyez sur Entr√©e",
    help="Le nouveau mot-cl√© sera ajout√© √† la liste des mots-cl√©s disponibles",
)
if new_keyword:
    if (
        new_keyword not in st.session_state.available_keywords
        and new_keyword not in st.session_state.selected_keywords
    ):
        st.session_state.available_keywords.append(new_keyword)
        st.session_state.selected_keywords.append(new_keyword)
        st.rerun()

# Utilisation de multiselect pour les mots-cl√©s
st.session_state.selected_keywords = st.multiselect(
    label="üè∑Ô∏è Mots-cl√©s pour la recherche :",
    options=st.session_state.available_keywords,
    default=st.session_state.selected_keywords,
    placeholder="Choisissez un ou plusieurs mots-cl√©s",
    help="S√©lectionnez les mots-cl√©s qui seront utilis√©s pour "
    "la recherche dans les urls des pages web",
)

# Ajout d'un bouton pour vider la table tarification_raw
if st.button(
    "üóëÔ∏è Vider la base de donn√©es",
    help="Attention: Cette action supprimera toutes les donn√©es de la table tarification_raw",
    type="secondary",
):
    try:
        engine = create_engine(get_postgres_cs())
        with engine.connect() as conn:
            conn.execute(text("TRUNCATE TABLE tarification_raw"))
            conn.commit()
        st.success("‚úÖ La table tarification_raw a √©t√© vid√©e avec succ√®s.")
    except Exception as e:
        st.error(f"‚ö†Ô∏è Erreur lors de la suppression des donn√©es: {str(e)}")

# Boutons existants pour d√©marrer/arr√™ter l'extraction
stop_button = st.button(
    "üõë Arr√™ter l'extraction",
    help="Cliquez pour arr√™ter l'extraction en cours",
    disabled=not st.session_state.get("is_crawling", False),
    on_click=toggle_crawling,
)

start_button = st.button(
    "üï∑Ô∏è Lancer l'extraction",
    help="Cliquez pour lancer l'extraction des donn√©es sur les sites web",
    disabled=st.session_state.get("is_crawling", False),
    on_click=toggle_crawling,
)

if start_button:
    with st.spinner("Extraction en cours..."):
        # Connexion √† la base de donn√©es
        engine = create_engine(get_postgres_cs())

        for data in DATA[:10]:
            url = data["site_web_principal"]
            n_siren_aom = data["n_siren_aom"]
            nom_aom = data["nom_aom"]

            with st.expander(f"N¬∞ SIREN AOM : {n_siren_aom}, {nom_aom}"):
                try:
                    # V√©rifier si l'URL a √©t√© scrap√©e r√©cemment (moins d'1h)
                    domain = urlparse(url).netloc
                    query = text(
                        """
                        SELECT * FROM tarification_raw 
                        WHERE url_source LIKE :domain_pattern 
                        AND date_scraping > :cutoff_time
                    """
                    )

                    with engine.connect() as conn:
                        result = conn.execute(
                            query,
                            {
                                "domain_pattern": f"%{domain}%",
                                "cutoff_time": datetime.now()
                                - timedelta(hours=1),
                            },
                        ).fetchone()

                    if result:
                        st.info(
                            f"‚è±Ô∏è URL {url} d√©j√† scrap√©e r√©cemment. Passage √† l'URL suivante."
                        )
                        continue

                    # Scraper l'URL
                    loop = st.session_state.loop
                    asyncio.set_event_loop(loop)
                    pages = loop.run_until_complete(
                        st.session_state.crawler_manager.fetch_content(
                            url,
                            st.session_state.selected_keywords,
                        )
                    )

                    # Sauvegarder les donn√©es dans la base de donn√©es
                    for page in pages:
                        with engine.connect() as conn:
                            conn.execute(
                                text(
                                    """
                                INSERT INTO tarification_raw 
                                (n_siren_aom, url_source, url_page, contenu_scrape)
                                VALUES (:n_siren_aom, :url_source, :url_page, :contenu_scrape)
                            """
                                ),
                                {
                                    "n_siren_aom": n_siren_aom,
                                    "url_source": url,
                                    "url_page": page.url,
                                    "contenu_scrape": page.markdown,
                                },
                            )
                            conn.commit()

                    # Cr√©er un onglet par page
                    tabs = st.tabs([f"Page {i+1}" for i in range(len(pages))])
                    for i, page in enumerate(pages):
                        with tabs[i]:
                            st.markdown(f"{page.url}")
                            st.markdown(page.markdown)

                except Exception as e:
                    st.error(f"‚ö†Ô∏è Une erreur est survenue : {str(e)}")
